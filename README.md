# LlamaNet_v1

A simple barebones (for now) Neural Network built using only the C++ Standard Library

To Implement:

Improved Batch Training

Allow for variable learning rate, leaky ReLU alphas and Huber Loss deltas

Allow for different activation functions 

ADAM adaptive learning rate

Nesterov Accelerated Gradient


Authored by: Alex Tan
