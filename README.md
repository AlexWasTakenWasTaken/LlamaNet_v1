# LlamaNet_v1

A simple barebones (for now) Neural Network built using only the C++ Standard Library

Current Features:

FCNN
Multi-input training
Huber Loss Cost function
Leaky ReLU activation function
Xavier Initialization


To Implement:

Improved Batch Training
Allow for variable learning rate, leaky ReLU alphas and Huber Loss deltas
Allow for different activation functions 
ADAM adaptive learning rate
Nesterov Accelerated Gradient

Authored by: Alex Tan (2024)
